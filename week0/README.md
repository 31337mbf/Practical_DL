
[__Lecture slides__](https://yadi.sk/i/LdEIut2z3MjPMv)
## Materials
* Russian lectures:
  * Lecture on basic neural networks (russian) - [url](https://yadi.sk/i/yyHZub6R3Ej5dV)
  * Adaptive optimization methods (russian) - [url](https://yadi.sk/i/SAGl44PS3EHZeK)
  * Backprop one formula at a time (russian) - [url](https://yadi.sk/i/0AuHgNsv3EHZhN)
* Lecture on neural nets and backprop (english) - [url](https://www.youtube.com/watch?v=uXt8qF2Zzfo)
* Stanford course in deep learning - [cs231](http://cs231n.github.io/linear-classify/)

## More on Stochastic Gradient Descent
  - [A blog post overview of gradient descent methods](http://ruder.io/optimizing-gradient-descent/)
  - [Cool interactive demo of momentum](http://distill.pub/2017/momentum/)
  - [wikipedia on SGD :)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), expecially the "extensions and variants" section
  - [RMSPROP video](https://www.youtube.com/watch?v=defQQqkXEfE)

## More on neural nets and backprop
  - Interactive [neural network playground](http://playground.tensorflow.org/) in your browser
  - [Backprop by cs231](http://cs231n.github.io/optimization-2/)
  - [Notation](http://cs231n.github.io/neural-networks-1/#nn)
  - pretty much all the module 1 of http://cs231n.github.io/


## Assignment

Your first assignment consists of two parts:
* Part I, where you implement simple logistic regression and train it with adaptive SGD modiffications
* Part II, where you implement a simple neural network in pure numpy

Both those notebooks are in current folder under respective names.
